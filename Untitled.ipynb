{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "file_path = 'pre/train_user.csv'\n",
    "\n",
    "\n",
    "# load dataset\n",
    "def load_data(path):\n",
    "    \n",
    "    # exclude the row of title \n",
    "    num = len(open(path,'r').readlines()) - 1 \n",
    "    data = np.empty((num,12),dtype=\"uint32\")\n",
    "    label = np.empty((num,1),dtype=\"uint8\")\n",
    "    \n",
    "    arr = genfromtxt(path, delimiter=',', dtype=\"uint32\", skip_header=1, usecols=(0,2,3,4,6,7,9,10,11,12,13,14,15))\n",
    "    \n",
    "    label = arr[:,0]\n",
    "    data = arr[:,1:]\n",
    "    \n",
    "    return data,label\n",
    "\n",
    "X,Y = load_data(file_path)\n",
    "\n",
    "#preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "#print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2999621, 12)\n",
      "(749906, 12)\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "\n",
    "train_size = int(X.shape[0]*train_split)\n",
    "test_size = X.shape[0] - train_size\n",
    "\n",
    "X_train, X_test = X[0:train_size,:], X[train_size:,:]\n",
    "y_train, y_test = Y[0:train_size], Y[train_size:,]\n",
    "\n",
    "#clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "#max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "#clf.score(X_test, y_test) \n",
    "#y_pred = clf.predict(X_test)\n",
    "index_train = np.where(y_train > 0)\n",
    "index_test = np.where(y_test > 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.461888\ttrain-logloss:0.461896\n",
      "Multiple eval metrics have been passed: 'train-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-logloss hasn't improved in 10 rounds.\n",
      "[1]\teval-logloss:0.335505\ttrain-logloss:0.335524\n",
      "[2]\teval-logloss:0.25764\ttrain-logloss:0.257678\n",
      "[3]\teval-logloss:0.207288\ttrain-logloss:0.207339\n",
      "[4]\teval-logloss:0.173945\ttrain-logloss:0.174008\n",
      "[5]\teval-logloss:0.151578\ttrain-logloss:0.151652\n",
      "[6]\teval-logloss:0.136515\ttrain-logloss:0.136598\n",
      "[7]\teval-logloss:0.126568\ttrain-logloss:0.126646\n",
      "[8]\teval-logloss:0.119703\ttrain-logloss:0.119818\n",
      "[9]\teval-logloss:0.115416\ttrain-logloss:0.115542\n",
      "[10]\teval-logloss:0.112545\ttrain-logloss:0.112647\n",
      "[11]\teval-logloss:0.110767\ttrain-logloss:0.11086\n",
      "[12]\teval-logloss:0.109649\ttrain-logloss:0.109748\n",
      "[13]\teval-logloss:0.108625\ttrain-logloss:0.108757\n",
      "[14]\teval-logloss:0.108104\ttrain-logloss:0.108206\n",
      "[15]\teval-logloss:0.107652\ttrain-logloss:0.107759\n",
      "[16]\teval-logloss:0.107204\ttrain-logloss:0.107325\n",
      "[17]\teval-logloss:0.106985\ttrain-logloss:0.107095\n",
      "[18]\teval-logloss:0.106469\ttrain-logloss:0.106583\n",
      "[19]\teval-logloss:0.106222\ttrain-logloss:0.106314\n",
      "[20]\teval-logloss:0.105824\ttrain-logloss:0.105905\n",
      "[21]\teval-logloss:0.105621\ttrain-logloss:0.105715\n",
      "[22]\teval-logloss:0.105531\ttrain-logloss:0.105603\n",
      "[23]\teval-logloss:0.105481\ttrain-logloss:0.10554\n",
      "[24]\teval-logloss:0.105421\ttrain-logloss:0.105449\n",
      "[25]\teval-logloss:0.1053\ttrain-logloss:0.105306\n",
      "[26]\teval-logloss:0.105189\ttrain-logloss:0.105192\n",
      "[27]\teval-logloss:0.105071\ttrain-logloss:0.10506\n",
      "[28]\teval-logloss:0.105039\ttrain-logloss:0.105015\n",
      "[29]\teval-logloss:0.104792\ttrain-logloss:0.104725\n",
      "[30]\teval-logloss:0.104693\ttrain-logloss:0.104618\n",
      "[31]\teval-logloss:0.104668\ttrain-logloss:0.104563\n",
      "[32]\teval-logloss:0.104642\ttrain-logloss:0.104527\n",
      "[33]\teval-logloss:0.104605\ttrain-logloss:0.104472\n",
      "[34]\teval-logloss:0.104335\ttrain-logloss:0.104149\n",
      "[35]\teval-logloss:0.104238\ttrain-logloss:0.104023\n",
      "[36]\teval-logloss:0.104161\ttrain-logloss:0.103932\n",
      "[37]\teval-logloss:0.104117\ttrain-logloss:0.10386\n",
      "[38]\teval-logloss:0.104019\ttrain-logloss:0.103752\n",
      "[39]\teval-logloss:0.103987\ttrain-logloss:0.103708\n",
      "[40]\teval-logloss:0.103855\ttrain-logloss:0.10357\n",
      "[41]\teval-logloss:0.103833\ttrain-logloss:0.103532\n",
      "[42]\teval-logloss:0.10382\ttrain-logloss:0.103501\n",
      "[43]\teval-logloss:0.103773\ttrain-logloss:0.103419\n",
      "[44]\teval-logloss:0.103712\ttrain-logloss:0.103347\n",
      "[45]\teval-logloss:0.103707\ttrain-logloss:0.103333\n",
      "[46]\teval-logloss:0.103637\ttrain-logloss:0.103235\n",
      "[47]\teval-logloss:0.103523\ttrain-logloss:0.103101\n",
      "[48]\teval-logloss:0.103451\ttrain-logloss:0.103019\n",
      "[49]\teval-logloss:0.103437\ttrain-logloss:0.103001\n",
      "[50]\teval-logloss:0.103424\ttrain-logloss:0.102947\n",
      "[51]\teval-logloss:0.103414\ttrain-logloss:0.102906\n",
      "[52]\teval-logloss:0.103401\ttrain-logloss:0.102857\n",
      "[53]\teval-logloss:0.103377\ttrain-logloss:0.1028\n",
      "[54]\teval-logloss:0.10333\ttrain-logloss:0.102732\n",
      "[55]\teval-logloss:0.103325\ttrain-logloss:0.10272\n",
      "[56]\teval-logloss:0.103326\ttrain-logloss:0.102714\n",
      "[57]\teval-logloss:0.103285\ttrain-logloss:0.102658\n",
      "[58]\teval-logloss:0.103258\ttrain-logloss:0.102599\n",
      "[59]\teval-logloss:0.103235\ttrain-logloss:0.102558\n",
      "[60]\teval-logloss:0.103132\ttrain-logloss:0.102438\n",
      "[61]\teval-logloss:0.103106\ttrain-logloss:0.102403\n",
      "[62]\teval-logloss:0.103101\ttrain-logloss:0.102395\n",
      "[63]\teval-logloss:0.103092\ttrain-logloss:0.10235\n",
      "[64]\teval-logloss:0.103043\ttrain-logloss:0.102288\n",
      "[65]\teval-logloss:0.102992\ttrain-logloss:0.102205\n",
      "[66]\teval-logloss:0.10298\ttrain-logloss:0.102174\n",
      "[67]\teval-logloss:0.102983\ttrain-logloss:0.102138\n",
      "[68]\teval-logloss:0.102981\ttrain-logloss:0.102119\n",
      "[69]\teval-logloss:0.10295\ttrain-logloss:0.102059\n",
      "[70]\teval-logloss:0.102928\ttrain-logloss:0.10202\n",
      "[71]\teval-logloss:0.102889\ttrain-logloss:0.101976\n",
      "[72]\teval-logloss:0.102891\ttrain-logloss:0.101943\n",
      "[73]\teval-logloss:0.102867\ttrain-logloss:0.101878\n",
      "[74]\teval-logloss:0.102867\ttrain-logloss:0.101874\n",
      "[75]\teval-logloss:0.102851\ttrain-logloss:0.10184\n",
      "[76]\teval-logloss:0.102834\ttrain-logloss:0.101814\n",
      "[77]\teval-logloss:0.102802\ttrain-logloss:0.101757\n",
      "[78]\teval-logloss:0.102806\ttrain-logloss:0.101735\n",
      "[79]\teval-logloss:0.102809\ttrain-logloss:0.101699\n",
      "[80]\teval-logloss:0.102777\ttrain-logloss:0.101653\n",
      "[81]\teval-logloss:0.102763\ttrain-logloss:0.101615\n",
      "[82]\teval-logloss:0.102772\ttrain-logloss:0.101589\n",
      "[83]\teval-logloss:0.102775\ttrain-logloss:0.101581\n",
      "[84]\teval-logloss:0.102777\ttrain-logloss:0.101577\n",
      "[85]\teval-logloss:0.102758\ttrain-logloss:0.101544\n",
      "[86]\teval-logloss:0.102744\ttrain-logloss:0.101508\n",
      "[87]\teval-logloss:0.102746\ttrain-logloss:0.101474\n",
      "[88]\teval-logloss:0.102737\ttrain-logloss:0.101444\n",
      "[89]\teval-logloss:0.102731\ttrain-logloss:0.101407\n",
      "[90]\teval-logloss:0.102713\ttrain-logloss:0.10138\n",
      "[91]\teval-logloss:0.102705\ttrain-logloss:0.10135\n",
      "[92]\teval-logloss:0.102703\ttrain-logloss:0.101327\n",
      "[93]\teval-logloss:0.102696\ttrain-logloss:0.101287\n",
      "[94]\teval-logloss:0.102699\ttrain-logloss:0.101268\n",
      "[95]\teval-logloss:0.102697\ttrain-logloss:0.101264\n",
      "[96]\teval-logloss:0.10269\ttrain-logloss:0.10124\n",
      "[97]\teval-logloss:0.10269\ttrain-logloss:0.101213\n",
      "[98]\teval-logloss:0.102689\ttrain-logloss:0.101198\n",
      "[99]\teval-logloss:0.102691\ttrain-logloss:0.101195\n",
      "[100]\teval-logloss:0.102692\ttrain-logloss:0.101188\n",
      "[101]\teval-logloss:0.102694\ttrain-logloss:0.101182\n",
      "[102]\teval-logloss:0.102682\ttrain-logloss:0.101162\n",
      "[103]\teval-logloss:0.102673\ttrain-logloss:0.101131\n",
      "[104]\teval-logloss:0.102671\ttrain-logloss:0.101125\n",
      "[105]\teval-logloss:0.10267\ttrain-logloss:0.101119\n",
      "[106]\teval-logloss:0.102673\ttrain-logloss:0.101116\n",
      "[107]\teval-logloss:0.102671\ttrain-logloss:0.101085\n",
      "[108]\teval-logloss:0.102673\ttrain-logloss:0.101072\n",
      "[109]\teval-logloss:0.102677\ttrain-logloss:0.101054\n",
      "[110]\teval-logloss:0.102645\ttrain-logloss:0.101014\n",
      "[111]\teval-logloss:0.102647\ttrain-logloss:0.101008\n",
      "[112]\teval-logloss:0.102644\ttrain-logloss:0.100979\n",
      "[113]\teval-logloss:0.102595\ttrain-logloss:0.100906\n",
      "[114]\teval-logloss:0.102587\ttrain-logloss:0.100872\n",
      "[115]\teval-logloss:0.102568\ttrain-logloss:0.100826\n",
      "[116]\teval-logloss:0.102565\ttrain-logloss:0.100801\n",
      "[117]\teval-logloss:0.102556\ttrain-logloss:0.10076\n",
      "[118]\teval-logloss:0.102545\ttrain-logloss:0.100739\n",
      "[119]\teval-logloss:0.102537\ttrain-logloss:0.100715\n",
      "[120]\teval-logloss:0.102539\ttrain-logloss:0.100691\n",
      "[121]\teval-logloss:0.102544\ttrain-logloss:0.10067\n",
      "[122]\teval-logloss:0.102554\ttrain-logloss:0.100654\n",
      "[123]\teval-logloss:0.102557\ttrain-logloss:0.100647\n",
      "[124]\teval-logloss:0.102558\ttrain-logloss:0.100626\n",
      "[125]\teval-logloss:0.102554\ttrain-logloss:0.100608\n",
      "[126]\teval-logloss:0.102555\ttrain-logloss:0.100595\n",
      "[127]\teval-logloss:0.102552\ttrain-logloss:0.100566\n",
      "[128]\teval-logloss:0.102552\ttrain-logloss:0.100546\n",
      "[129]\teval-logloss:0.102548\ttrain-logloss:0.100521\n",
      "[130]\teval-logloss:0.102545\ttrain-logloss:0.100503\n",
      "[131]\teval-logloss:0.102548\ttrain-logloss:0.1005\n",
      "[132]\teval-logloss:0.102532\ttrain-logloss:0.10046\n",
      "[133]\teval-logloss:0.102527\ttrain-logloss:0.100446\n",
      "[134]\teval-logloss:0.102528\ttrain-logloss:0.100424\n",
      "[135]\teval-logloss:0.102523\ttrain-logloss:0.100405\n",
      "[136]\teval-logloss:0.102522\ttrain-logloss:0.100382\n",
      "[137]\teval-logloss:0.10251\ttrain-logloss:0.100345\n",
      "[138]\teval-logloss:0.102493\ttrain-logloss:0.100309\n",
      "[139]\teval-logloss:0.102481\ttrain-logloss:0.100282\n",
      "[140]\teval-logloss:0.102473\ttrain-logloss:0.100255\n",
      "[141]\teval-logloss:0.102464\ttrain-logloss:0.100218\n",
      "[142]\teval-logloss:0.102459\ttrain-logloss:0.100205\n",
      "[143]\teval-logloss:0.102422\ttrain-logloss:0.100155\n",
      "[144]\teval-logloss:0.102424\ttrain-logloss:0.100138\n",
      "[145]\teval-logloss:0.102404\ttrain-logloss:0.100108\n",
      "[146]\teval-logloss:0.102406\ttrain-logloss:0.100079\n",
      "[147]\teval-logloss:0.102408\ttrain-logloss:0.100064\n",
      "[148]\teval-logloss:0.102368\ttrain-logloss:0.099997\n",
      "[149]\teval-logloss:0.102354\ttrain-logloss:0.099961\n",
      "[150]\teval-logloss:0.102356\ttrain-logloss:0.099945\n",
      "[151]\teval-logloss:0.102349\ttrain-logloss:0.099903\n",
      "[152]\teval-logloss:0.102342\ttrain-logloss:0.099876\n",
      "[153]\teval-logloss:0.102347\ttrain-logloss:0.099855\n",
      "[154]\teval-logloss:0.102349\ttrain-logloss:0.099847\n",
      "[155]\teval-logloss:0.102351\ttrain-logloss:0.099827\n",
      "[156]\teval-logloss:0.102342\ttrain-logloss:0.099807\n",
      "[157]\teval-logloss:0.102334\ttrain-logloss:0.099788\n",
      "[158]\teval-logloss:0.102331\ttrain-logloss:0.099768\n",
      "[159]\teval-logloss:0.102333\ttrain-logloss:0.099765\n",
      "[160]\teval-logloss:0.102327\ttrain-logloss:0.099752\n",
      "[161]\teval-logloss:0.102327\ttrain-logloss:0.099736\n",
      "[162]\teval-logloss:0.102324\ttrain-logloss:0.099713\n",
      "[163]\teval-logloss:0.102328\ttrain-logloss:0.099699\n",
      "[164]\teval-logloss:0.10232\ttrain-logloss:0.099671\n",
      "[165]\teval-logloss:0.102318\ttrain-logloss:0.09965\n",
      "[166]\teval-logloss:0.102312\ttrain-logloss:0.099629\n",
      "[167]\teval-logloss:0.102289\ttrain-logloss:0.09959\n",
      "[168]\teval-logloss:0.102294\ttrain-logloss:0.099575\n",
      "[169]\teval-logloss:0.102299\ttrain-logloss:0.099558\n",
      "[170]\teval-logloss:0.102299\ttrain-logloss:0.099548\n",
      "[171]\teval-logloss:0.102299\ttrain-logloss:0.099536\n",
      "[172]\teval-logloss:0.102299\ttrain-logloss:0.099534\n",
      "[173]\teval-logloss:0.102296\ttrain-logloss:0.099511\n",
      "[174]\teval-logloss:0.102298\ttrain-logloss:0.099494\n",
      "[175]\teval-logloss:0.102283\ttrain-logloss:0.099469\n",
      "[176]\teval-logloss:0.102281\ttrain-logloss:0.099445\n",
      "[177]\teval-logloss:0.102274\ttrain-logloss:0.099411\n",
      "[178]\teval-logloss:0.102271\ttrain-logloss:0.099383\n",
      "[179]\teval-logloss:0.102273\ttrain-logloss:0.099361\n",
      "[180]\teval-logloss:0.102268\ttrain-logloss:0.099342\n",
      "[181]\teval-logloss:0.102254\ttrain-logloss:0.099315\n",
      "[182]\teval-logloss:0.102241\ttrain-logloss:0.099282\n",
      "[183]\teval-logloss:0.102245\ttrain-logloss:0.099263\n",
      "[184]\teval-logloss:0.102247\ttrain-logloss:0.099252\n",
      "[185]\teval-logloss:0.102247\ttrain-logloss:0.099243\n",
      "[186]\teval-logloss:0.102249\ttrain-logloss:0.099232\n",
      "[187]\teval-logloss:0.102243\ttrain-logloss:0.099211\n",
      "[188]\teval-logloss:0.102243\ttrain-logloss:0.099192\n",
      "[189]\teval-logloss:0.102246\ttrain-logloss:0.099167\n",
      "[190]\teval-logloss:0.102249\ttrain-logloss:0.099157\n",
      "[191]\teval-logloss:0.10225\ttrain-logloss:0.099151\n",
      "[192]\teval-logloss:0.102253\ttrain-logloss:0.099148\n",
      "[193]\teval-logloss:0.10225\ttrain-logloss:0.099136\n",
      "[194]\teval-logloss:0.102251\ttrain-logloss:0.099118\n",
      "[195]\teval-logloss:0.102253\ttrain-logloss:0.099103\n",
      "[196]\teval-logloss:0.102252\ttrain-logloss:0.099089\n",
      "[197]\teval-logloss:0.102254\ttrain-logloss:0.099087\n",
      "[198]\teval-logloss:0.102255\ttrain-logloss:0.099073\n",
      "[199]\teval-logloss:0.102251\ttrain-logloss:0.09905\n",
      "[200]\teval-logloss:0.102255\ttrain-logloss:0.099025\n",
      "[201]\teval-logloss:0.102258\ttrain-logloss:0.099007\n",
      "[202]\teval-logloss:0.102261\ttrain-logloss:0.098983\n",
      "[203]\teval-logloss:0.102255\ttrain-logloss:0.098959\n",
      "[204]\teval-logloss:0.102257\ttrain-logloss:0.098942\n",
      "[205]\teval-logloss:0.102256\ttrain-logloss:0.098927\n",
      "[206]\teval-logloss:0.102262\ttrain-logloss:0.098896\n",
      "[207]\teval-logloss:0.102263\ttrain-logloss:0.098875\n",
      "[208]\teval-logloss:0.102271\ttrain-logloss:0.098858\n",
      "[209]\teval-logloss:0.102267\ttrain-logloss:0.098836\n",
      "[210]\teval-logloss:0.102266\ttrain-logloss:0.098833\n",
      "[211]\teval-logloss:0.102245\ttrain-logloss:0.098803\n",
      "[212]\teval-logloss:0.102243\ttrain-logloss:0.09878\n",
      "[213]\teval-logloss:0.10225\ttrain-logloss:0.098754\n",
      "[214]\teval-logloss:0.10225\ttrain-logloss:0.098752\n",
      "[215]\teval-logloss:0.10225\ttrain-logloss:0.09875\n",
      "[216]\teval-logloss:0.102249\ttrain-logloss:0.098745\n",
      "[217]\teval-logloss:0.102252\ttrain-logloss:0.098723\n",
      "[218]\teval-logloss:0.102258\ttrain-logloss:0.098702\n",
      "[219]\teval-logloss:0.102254\ttrain-logloss:0.098674\n",
      "[220]\teval-logloss:0.102256\ttrain-logloss:0.09866\n",
      "[221]\teval-logloss:0.102263\ttrain-logloss:0.09864\n",
      "[222]\teval-logloss:0.102266\ttrain-logloss:0.098614\n",
      "[223]\teval-logloss:0.102272\ttrain-logloss:0.098596\n",
      "[224]\teval-logloss:0.102273\ttrain-logloss:0.098592\n",
      "[225]\teval-logloss:0.102261\ttrain-logloss:0.098569\n",
      "[226]\teval-logloss:0.102271\ttrain-logloss:0.098546\n",
      "[227]\teval-logloss:0.102273\ttrain-logloss:0.098536\n",
      "[228]\teval-logloss:0.102272\ttrain-logloss:0.098518\n",
      "[229]\teval-logloss:0.102271\ttrain-logloss:0.098501\n",
      "[230]\teval-logloss:0.10227\ttrain-logloss:0.09848\n",
      "[231]\teval-logloss:0.102267\ttrain-logloss:0.098463\n",
      "[232]\teval-logloss:0.10227\ttrain-logloss:0.09845\n",
      "[233]\teval-logloss:0.102272\ttrain-logloss:0.098445\n",
      "[234]\teval-logloss:0.102274\ttrain-logloss:0.098439\n",
      "[235]\teval-logloss:0.102272\ttrain-logloss:0.098425\n",
      "[236]\teval-logloss:0.102277\ttrain-logloss:0.098411\n",
      "[237]\teval-logloss:0.102276\ttrain-logloss:0.098391\n",
      "[238]\teval-logloss:0.10228\ttrain-logloss:0.098377\n",
      "[239]\teval-logloss:0.102279\ttrain-logloss:0.098366\n",
      "[240]\teval-logloss:0.102284\ttrain-logloss:0.098352\n",
      "[241]\teval-logloss:0.102285\ttrain-logloss:0.098333\n",
      "[242]\teval-logloss:0.102272\ttrain-logloss:0.098309\n",
      "[243]\teval-logloss:0.102273\ttrain-logloss:0.098305\n",
      "[244]\teval-logloss:0.102282\ttrain-logloss:0.09828\n",
      "[245]\teval-logloss:0.102287\ttrain-logloss:0.098266\n",
      "[246]\teval-logloss:0.102287\ttrain-logloss:0.098256\n",
      "[247]\teval-logloss:0.102279\ttrain-logloss:0.098225\n",
      "[248]\teval-logloss:0.10228\ttrain-logloss:0.09821\n",
      "[249]\teval-logloss:0.102285\ttrain-logloss:0.098192\n",
      "[250]\teval-logloss:0.102286\ttrain-logloss:0.09819\n",
      "[251]\teval-logloss:0.10228\ttrain-logloss:0.098169\n",
      "[252]\teval-logloss:0.102278\ttrain-logloss:0.098138\n",
      "[253]\teval-logloss:0.10227\ttrain-logloss:0.098116\n",
      "[254]\teval-logloss:0.102266\ttrain-logloss:0.098089\n",
      "[255]\teval-logloss:0.102268\ttrain-logloss:0.098072\n",
      "[256]\teval-logloss:0.102257\ttrain-logloss:0.098049\n",
      "[257]\teval-logloss:0.102249\ttrain-logloss:0.098014\n",
      "[258]\teval-logloss:0.10225\ttrain-logloss:0.097995\n",
      "[259]\teval-logloss:0.102246\ttrain-logloss:0.097965\n",
      "[260]\teval-logloss:0.102229\ttrain-logloss:0.097944\n",
      "[261]\teval-logloss:0.102228\ttrain-logloss:0.097929\n",
      "[262]\teval-logloss:0.102232\ttrain-logloss:0.097909\n",
      "[263]\teval-logloss:0.102235\ttrain-logloss:0.097901\n",
      "[264]\teval-logloss:0.102236\ttrain-logloss:0.09789\n",
      "[265]\teval-logloss:0.102242\ttrain-logloss:0.097865\n",
      "[266]\teval-logloss:0.102246\ttrain-logloss:0.097845\n",
      "[267]\teval-logloss:0.10225\ttrain-logloss:0.09782\n",
      "[268]\teval-logloss:0.102253\ttrain-logloss:0.097809\n",
      "[269]\teval-logloss:0.102248\ttrain-logloss:0.097788\n",
      "[270]\teval-logloss:0.102251\ttrain-logloss:0.097772\n",
      "[271]\teval-logloss:0.102253\ttrain-logloss:0.097763\n",
      "[272]\teval-logloss:0.102252\ttrain-logloss:0.097753\n",
      "[273]\teval-logloss:0.102254\ttrain-logloss:0.097752\n",
      "[274]\teval-logloss:0.102257\ttrain-logloss:0.097729\n",
      "[275]\teval-logloss:0.102258\ttrain-logloss:0.097717\n",
      "[276]\teval-logloss:0.102257\ttrain-logloss:0.097705\n",
      "[277]\teval-logloss:0.102253\ttrain-logloss:0.09769\n",
      "[278]\teval-logloss:0.102251\ttrain-logloss:0.097672\n",
      "[279]\teval-logloss:0.102252\ttrain-logloss:0.097661\n",
      "[280]\teval-logloss:0.102252\ttrain-logloss:0.097645\n",
      "[281]\teval-logloss:0.102247\ttrain-logloss:0.097628\n",
      "[282]\teval-logloss:0.102247\ttrain-logloss:0.097619\n",
      "[283]\teval-logloss:0.102257\ttrain-logloss:0.097594\n",
      "[284]\teval-logloss:0.10226\ttrain-logloss:0.097578\n",
      "[285]\teval-logloss:0.102263\ttrain-logloss:0.097568\n",
      "[286]\teval-logloss:0.102262\ttrain-logloss:0.097547\n",
      "[287]\teval-logloss:0.102261\ttrain-logloss:0.09754\n",
      "[288]\teval-logloss:0.102261\ttrain-logloss:0.097539\n",
      "[289]\teval-logloss:0.102265\ttrain-logloss:0.097523\n",
      "[290]\teval-logloss:0.102266\ttrain-logloss:0.097523\n",
      "[291]\teval-logloss:0.102267\ttrain-logloss:0.09752\n",
      "[292]\teval-logloss:0.102268\ttrain-logloss:0.097502\n",
      "[293]\teval-logloss:0.102269\ttrain-logloss:0.097485\n",
      "[294]\teval-logloss:0.102272\ttrain-logloss:0.097475\n",
      "[295]\teval-logloss:0.102278\ttrain-logloss:0.097461\n",
      "[296]\teval-logloss:0.102281\ttrain-logloss:0.097442\n",
      "[297]\teval-logloss:0.102286\ttrain-logloss:0.09743\n",
      "[298]\teval-logloss:0.10229\ttrain-logloss:0.097406\n",
      "[299]\teval-logloss:0.102286\ttrain-logloss:0.097381\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "data_train, data_test = X_train, X_test\n",
    "label_trian, label_test = y_train, y_test\n",
    "\n",
    "#get data\n",
    "dtrain = xgb.DMatrix(data_train, label=label_trian, missing = -999.0)\n",
    "dtest = xgb.DMatrix(data_test, label=label_test, missing = -999.0)\n",
    "#set params\n",
    "params = {'bst:max_depth':2, 'bst:eta':1, 'silent':False, 'objective':'binary:logistic','nthread':6,'eval_metric':'logloss'}\n",
    "\n",
    "evallist  = [(dtest,'eval'),(dtrain,'train')]\n",
    "#train\n",
    "num_round = 262\n",
    "stopping_rounds = 10\n",
    "bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=stopping_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst.save_model('size80_300.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00487088  0.00154489  0.00287563 ...,  0.0088817   0.02877679\n",
      "  0.00366664]\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10228572927116146"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#print(data_test.shape)\n",
    "y_pred = bst.predict(dtest)\n",
    "print(y_pred)\n",
    "print(label_test)\n",
    "metrics.log_loss(label_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load real dataset\n",
    "def load_pre_data(path):\n",
    "    \n",
    "    # exclude the row of title \n",
    "    num = len(open(path,'r').readlines()) - 1 \n",
    "    data = np.empty((num,13),dtype=\"uint32\")\n",
    "    instance = np.empty((num,1),dtype=\"uint32\")\n",
    "    \n",
    "    arr = genfromtxt(path, delimiter=',', dtype=\"uint32\", skip_header=1, usecols=(0,2,3,4,5,6,7,9,10,11,12,13,14,15))\n",
    "    \n",
    "    instance = arr[:,0]\n",
    "    data = arr[:,1:]\n",
    "\n",
    "    return instance,data\n",
    "\n",
    "ins,realX = load_pre_data(\"pre/test_user.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dreal = xgb.DMatrix(realX, missing = -999.0)\n",
    "Y_real = bst.predict(dreal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = np.column_stack((ins,Y_real))\n",
    "np.savetxt(\"pred3.csv\", res,fmt='%.9f',delimiter=',',header=\"instanceID,prob\")\n",
    "#print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "def MLP(dim):\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = MLP(6)\n",
    "\n",
    "model.fit(X_train, y_train,epochs=20,batch_size=2500)\n",
    "\n",
    "#score = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "#plot_model(model, to_file='model.png')\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
